# -*- coding: utf-8 -*-
"""Voice reco.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15oTvN2GsW0963DG9WAItP1c7RaNPCEsw
"""

!pip install tensorflow numpy librosa

import tensorflow as tf
from tensorflow.keras import layers
import numpy as np
import librosa

# Define the model
model = tf.keras.Sequential([
    layers.Input(shape=(8000, 1)),
    layers.Conv1D(8, 13, padding='valid', activation='relu', strides=1),
    layers.MaxPooling1D(3),
    layers.Conv1D(16, 11, padding='valid', activation='relu', strides=1),
    layers.MaxPooling1D(3),
    layers.Conv1D(32, 9, padding='valid', activation='relu', strides=1),
    layers.MaxPooling1D(3),
    layers.GlobalAveragePooling1D(),
    layers.Dense(20, activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.summary()

def load_and_preprocess_audio(file_path):
    # Load the audio file using librosa
    audio, _ = librosa.load(file_path, sr=8000)

    # Pad or truncate the audio to a fixed length (8000 samples in this example)
    audio = np.pad(audio, (0, max(0, 8000 - len(audio))), 'constant')

    return np.expand_dims(audio, axis=1)  # Add an extra dimension for the channel

# Example usage:
file_path = 'file_example_WAV_1MG.wav'
audio_data = load_and_preprocess_audio(file_path)

# Assuming you have a list of file paths and corresponding labels
file_paths = ['file_example_WAV_1MG.wav']  # List of file paths
labels = [...]      # Corresponding labels (e.g., 'yes', 'no', 'up', 'down', etc.)

# Convert labels to numerical values
label_to_index = {label: i for i, label in enumerate(set(labels))}
numerical_labels = [label_to_index[label] for label in labels]

# Load and preprocess audio data
audio_data = np.array([load_and_preprocess_audio(file_path) for file_path in file_paths])
numerical_labels = np.array(numerical_labels)

# Train the model
model.fit(audio_data, numerical_labels, epochs=10, batch_size=32, validation_split=0.2)

# Assuming you have a new audio file for inference
new_file_path = 'path/to/your/new_audio_file.wav'
new_audio_data = load_and_preprocess_audio(new_file_path)

# Perform inference
predictions = model.predict(np.array([new_audio_data]))
predicted_class = np.argmax(predictions)

# Convert numerical class back to label
index_to_label = {i: label for label, i in label_to_index.items()}
predicted_label = index_to_label[predicted_class]

print(f"Predicted label: {predicted_label}")